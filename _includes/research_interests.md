<h1 id="news"></h1>

<h2 style="margin: 60px 0px 10px;">Research Interests</h2>

<p>
My research is driven by the intriguing capabilities of large-scale models, primarily focusing on two areas:

<ul>
  <li><strong><a href="https://aclanthology.org/2022.acl-long.581.pdf">Representation Probing</a></strong>: I delve into the inner workings of these advanced models to understand how they represent and process information. My <a href="https://arxiv.org/pdf/2310.17041.pdf">publication</a> on EMNLP 2023 presents innovative metrics  for probing language models. The study demonstrates these metrics can match or surpass full model fine-tuning across most GLUE and SuperGLUE tasks, with a potential 25% reduction in training time. 
  </li>

  <li><strong><a href="http://rewriting.csail.mit.edu/">Model Editing</a></strong>: Building on these insights, I explore whether we can refine these models for enhanced performance and transparency. Presently, I am advancing geometry-aware techniques for the editing of diffusion models, focusing on integrating spatial understanding into model refinements.

  </li>
</ul>

  These pursuits aim to unravel the complexities of AI to ensure its profound competencies are not just an illusion but a measurable reality.
</p>
